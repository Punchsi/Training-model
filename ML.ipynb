{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare the data in one-against-the-rest strategy.This can be done by converting the \"Species\" column into 3 binary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00         9\n",
      "Iris-versicolor       0.88      1.00      0.93         7\n",
      " Iris-virginica       1.00      0.93      0.96        14\n",
      "\n",
      "       accuracy                           0.97        30\n",
      "      macro avg       0.96      0.98      0.97        30\n",
      "   weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sirintip\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load flower information\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\sirintip\\\\Downloads\\\\IRIS.csv\")\n",
    "\n",
    "# Separate the data into training and test data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, :-1], data.iloc[:, -1], test_size=0.2)\n",
    "\n",
    "# Create model\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# Train model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Formulate the error function of the logistic regression with ridge regularization criterion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(w, X, y, lambda_):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate the error function of the modified logistic model with regulations\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w: Vector, model weight\n",
    "    X: Training information\n",
    "    y: target value of training data\n",
    "    lambda_: Regulatory coefficient\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss: model error value\n",
    "    \"\"\"\n",
    "\n",
    "    logit = w.T @ X\n",
    "    loss = -np.mean(y * np.log(logit) + (1 - y) * np.log(1 - logit))\n",
    "    loss += lambda_ * np.sum(w ** 2)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sirintip\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import category_encoders as ce\n",
    "\n",
    "def load_data():\n",
    "    # Load IRIS data set\n",
    "    data = pd.read_csv(\"C:\\\\Users\\\\sirintip\\\\Downloads\\\\IRIS.csv\")\n",
    "\n",
    "    # Extract only numeric columns for training\n",
    "    numeric_columns = data.select_dtypes(include=[np.number])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(numeric_columns, data[\"species\"], test_size=0.25)\n",
    "\n",
    "    # Encode target values using LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "    return X_train, y_train_encoded, X_test, y_test_encoded\n",
    "\n",
    "\n",
    "def evaluate(model, X_test, y_test):\n",
    "\n",
    "\n",
    "    # Predict the target value of the test data\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Calculate model accuracy\n",
    "    accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "    # Return the model accuracy\n",
    "    return accuracy\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Set various parameters\n",
    "    lambda_ = 0.01\n",
    "\n",
    "    # Load training and test data\n",
    "    X_train, y_train_encoded, X_test, y_test_encoded = load_data()\n",
    "\n",
    "    # Convert target values to float\n",
    "    y_train_encoded = np.ravel(y_train_encoded)\n",
    "\n",
    "    # Practice model\n",
    "    model = LogisticRegression(penalty=\"l2\", C=1 / lambda_)\n",
    "\n",
    "    # Convert target values to one-hot encoding\n",
    "    # y_train_encoded = encoder.fit_transform(y_train)\n",
    "\n",
    "    model.fit(X_train, y_train_encoded)\n",
    "\n",
    "    # Assess the efficiency of the model\n",
    "    accuracy = evaluate(model, X_test, y_test_encoded)\n",
    "\n",
    "    # Print the model accuracy\n",
    "    print(\"Model accuracy:\", accuracy)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Derive the gradient of the error function by deriving the partial derivative of the error function in Task 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.25        1.42857143  1.25       -1.11111111]\n"
     ]
    }
   ],
   "source": [
    "def grad_log_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of the log loss function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: True label\n",
    "    y_hat: Predicted label\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Gradient\n",
    "    \"\"\"\n",
    "\n",
    "    return (y - y_hat) / (y_hat * (1 - y_hat))\n",
    "\n",
    "\n",
    "# Test\n",
    "y = np.array([0, 1, 1, 0])\n",
    "y_hat = np.array([0.2, 0.7, 0.8, 0.1])\n",
    "\n",
    "grad = grad_log_loss(y, y_hat)\n",
    "print(grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Implement the gradient descent using all of the dataset in each iteration. (Use equation from Task 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.7353444  7.62805408]\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(X, y, learning_rate, max_iter):\n",
    "    \"\"\"\n",
    "    Implement gradient descent algorithm\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: Feature matrix\n",
    "    y: True label\n",
    "    learning_rate: Learning rate\n",
    "    max_iter: Maximum number of iterations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Weights\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize weights\n",
    "    w = np.zeros(X.shape[1])\n",
    "\n",
    "    # Loop over iterations\n",
    "    for _ in range(max_iter):\n",
    "        # Calculate gradient\n",
    "        gradient = np.zeros(X.shape[1])\n",
    "        for i in range(X.shape[0]):\n",
    "            gradient += (y[i] - sigmoid(X[i] @ w)) * X[i]\n",
    "\n",
    "        # Update weights\n",
    "        w = w - learning_rate * gradient\n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "# Test\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "y = np.array([0, 1, 0])\n",
    "\n",
    "# Train model\n",
    "w = gradient_descent(X, y, 0.01, 100)\n",
    "\n",
    "# Print weights\n",
    "print(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Implement the stochastic gradient descent using the subset of dataset in each iteration. (Use equation from Task 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, lambda_, alpha, max_iter, batch_size):\n",
    "    \"\"\"\n",
    "    Use randomized gradient values to practice modified logistic models with regulations\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: Training information\n",
    "    y: target value of training data\n",
    "    lambda_: Regulatory coefficient\n",
    "    alpha: learning rate\n",
    "    max_iter: number of devaluation cycles\n",
    "    batch_size: The size of the data set used in each devaluation cycle\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    w: Vector, model weight\n",
    "    loss_history: History of model error values in each devaluation cycle\n",
    "    \"\"\"\n",
    "\n",
    "    w = np.zeros(X.shape[1])\n",
    "    loss_history = []\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        for j in range(0, X.shape[0], batch_size):\n",
    "            grad = gradient(w, X[j:j + batch_size], y[j:j + batch_size], lambda_)\n",
    "            w = w - alpha * grad\n",
    "\n",
    "        loss = loss_function(w, X, y, lambda_)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "    return w, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05618212 -0.10321414]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def grad_log_loss(y, y_hat):\n",
    "    \"\"\"Calculate the gradient of the log loss function.\n",
    "\n",
    "    Args:\n",
    "        y: True label.\n",
    "        y_hat: Predicted label.\n",
    "\n",
    "    Returns:\n",
    "        Gradient.\n",
    "    \"\"\"\n",
    "    return -(y - y_hat)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Calculate the sigmoid function.\n",
    "\n",
    "    Args:\n",
    "        x: Input.\n",
    "\n",
    "    Returns:\n",
    "        Output.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def stochastic_gradient_descent(X, y, learning_rate, max_iter):\n",
    "    \"\"\"Implement stochastic gradient descent algorithm.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix.\n",
    "        y: True label.\n",
    "        learning_rate: Learning rate.\n",
    "        max_iter: Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        Weights.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize weights.\n",
    "    w = np.zeros(X.shape[1])\n",
    "\n",
    "    # Loop over iterations.\n",
    "    for _ in range(max_iter):\n",
    "        # Randomly select a subset of data.\n",
    "        idx = np.random.choice(X.shape[0], size=min(10, X.shape[0]), replace=False)\n",
    "        X_sub = X[idx]\n",
    "        y_sub = y[idx]\n",
    "\n",
    "        # Calculate gradient.\n",
    "        gradient = np.mean(X_sub * (sigmoid(X_sub @ w) - y_sub)[:, np.newaxis], axis=0)\n",
    "\n",
    "        # Update weights.\n",
    "        w = w - learning_rate * gradient\n",
    "\n",
    "    return w\n",
    "\n",
    "# Test\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "y = np.array([0, 1, 0])\n",
    "\n",
    "# Train model\n",
    "w = stochastic_gradient_descent(X, y, 0.01, 100)\n",
    "\n",
    "# Print weights\n",
    "print(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.Test to see the effect of l on the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2 'Iris-setosa']\n",
      " [4.9 3.0 1.4 0.2 'Iris-setosa']\n",
      " [4.7 3.2 1.3 0.2 'Iris-setosa']\n",
      " [4.6 3.1 1.5 0.2 'Iris-setosa']\n",
      " [5.0 3.6 1.4 0.2 'Iris-setosa']\n",
      " [5.4 3.9 1.7 0.4 'Iris-setosa']\n",
      " [4.6 3.4 1.4 0.3 'Iris-setosa']\n",
      " [5.0 3.4 1.5 0.2 'Iris-setosa']\n",
      " [4.4 2.9 1.4 0.2 'Iris-setosa']\n",
      " [4.9 3.1 1.5 0.1 'Iris-setosa']\n",
      " [5.4 3.7 1.5 0.2 'Iris-setosa']\n",
      " [4.8 3.4 1.6 0.2 'Iris-setosa']\n",
      " [4.8 3.0 1.4 0.1 'Iris-setosa']\n",
      " [4.3 3.0 1.1 0.1 'Iris-setosa']\n",
      " [5.8 4.0 1.2 0.2 'Iris-setosa']\n",
      " [5.7 4.4 1.5 0.4 'Iris-setosa']\n",
      " [5.4 3.9 1.3 0.4 'Iris-setosa']\n",
      " [5.1 3.5 1.4 0.3 'Iris-setosa']\n",
      " [5.7 3.8 1.7 0.3 'Iris-setosa']\n",
      " [5.1 3.8 1.5 0.3 'Iris-setosa']\n",
      " [5.4 3.4 1.7 0.2 'Iris-setosa']\n",
      " [5.1 3.7 1.5 0.4 'Iris-setosa']\n",
      " [4.6 3.6 1.0 0.2 'Iris-setosa']\n",
      " [5.1 3.3 1.7 0.5 'Iris-setosa']\n",
      " [4.8 3.4 1.9 0.2 'Iris-setosa']\n",
      " [5.0 3.0 1.6 0.2 'Iris-setosa']\n",
      " [5.0 3.4 1.6 0.4 'Iris-setosa']\n",
      " [5.2 3.5 1.5 0.2 'Iris-setosa']\n",
      " [5.2 3.4 1.4 0.2 'Iris-setosa']\n",
      " [4.7 3.2 1.6 0.2 'Iris-setosa']\n",
      " [4.8 3.1 1.6 0.2 'Iris-setosa']\n",
      " [5.4 3.4 1.5 0.4 'Iris-setosa']\n",
      " [5.2 4.1 1.5 0.1 'Iris-setosa']\n",
      " [5.5 4.2 1.4 0.2 'Iris-setosa']\n",
      " [4.9 3.1 1.5 0.1 'Iris-setosa']\n",
      " [5.0 3.2 1.2 0.2 'Iris-setosa']\n",
      " [5.5 3.5 1.3 0.2 'Iris-setosa']\n",
      " [4.9 3.1 1.5 0.1 'Iris-setosa']\n",
      " [4.4 3.0 1.3 0.2 'Iris-setosa']\n",
      " [5.1 3.4 1.5 0.2 'Iris-setosa']\n",
      " [5.0 3.5 1.3 0.3 'Iris-setosa']\n",
      " [4.5 2.3 1.3 0.3 'Iris-setosa']\n",
      " [4.4 3.2 1.3 0.2 'Iris-setosa']\n",
      " [5.0 3.5 1.6 0.6 'Iris-setosa']\n",
      " [5.1 3.8 1.9 0.4 'Iris-setosa']\n",
      " [4.8 3.0 1.4 0.3 'Iris-setosa']\n",
      " [5.1 3.8 1.6 0.2 'Iris-setosa']\n",
      " [4.6 3.2 1.4 0.2 'Iris-setosa']\n",
      " [5.3 3.7 1.5 0.2 'Iris-setosa']\n",
      " [5.0 3.3 1.4 0.2 'Iris-setosa']\n",
      " [7.0 3.2 4.7 1.4 'Iris-versicolor']\n",
      " [6.4 3.2 4.5 1.5 'Iris-versicolor']\n",
      " [6.9 3.1 4.9 1.5 'Iris-versicolor']\n",
      " [5.5 2.3 4.0 1.3 'Iris-versicolor']\n",
      " [6.5 2.8 4.6 1.5 'Iris-versicolor']\n",
      " [5.7 2.8 4.5 1.3 'Iris-versicolor']\n",
      " [6.3 3.3 4.7 1.6 'Iris-versicolor']\n",
      " [4.9 2.4 3.3 1.0 'Iris-versicolor']\n",
      " [6.6 2.9 4.6 1.3 'Iris-versicolor']\n",
      " [5.2 2.7 3.9 1.4 'Iris-versicolor']\n",
      " [5.0 2.0 3.5 1.0 'Iris-versicolor']\n",
      " [5.9 3.0 4.2 1.5 'Iris-versicolor']\n",
      " [6.0 2.2 4.0 1.0 'Iris-versicolor']\n",
      " [6.1 2.9 4.7 1.4 'Iris-versicolor']\n",
      " [5.6 2.9 3.6 1.3 'Iris-versicolor']\n",
      " [6.7 3.1 4.4 1.4 'Iris-versicolor']\n",
      " [5.6 3.0 4.5 1.5 'Iris-versicolor']\n",
      " [5.8 2.7 4.1 1.0 'Iris-versicolor']\n",
      " [6.2 2.2 4.5 1.5 'Iris-versicolor']\n",
      " [5.6 2.5 3.9 1.1 'Iris-versicolor']\n",
      " [5.9 3.2 4.8 1.8 'Iris-versicolor']\n",
      " [6.1 2.8 4.0 1.3 'Iris-versicolor']\n",
      " [6.3 2.5 4.9 1.5 'Iris-versicolor']\n",
      " [6.1 2.8 4.7 1.2 'Iris-versicolor']\n",
      " [6.4 2.9 4.3 1.3 'Iris-versicolor']\n",
      " [6.6 3.0 4.4 1.4 'Iris-versicolor']\n",
      " [6.8 2.8 4.8 1.4 'Iris-versicolor']\n",
      " [6.7 3.0 5.0 1.7 'Iris-versicolor']\n",
      " [6.0 2.9 4.5 1.5 'Iris-versicolor']\n",
      " [5.7 2.6 3.5 1.0 'Iris-versicolor']\n",
      " [5.5 2.4 3.8 1.1 'Iris-versicolor']\n",
      " [5.5 2.4 3.7 1.0 'Iris-versicolor']\n",
      " [5.8 2.7 3.9 1.2 'Iris-versicolor']\n",
      " [6.0 2.7 5.1 1.6 'Iris-versicolor']\n",
      " [5.4 3.0 4.5 1.5 'Iris-versicolor']\n",
      " [6.0 3.4 4.5 1.6 'Iris-versicolor']\n",
      " [6.7 3.1 4.7 1.5 'Iris-versicolor']\n",
      " [6.3 2.3 4.4 1.3 'Iris-versicolor']\n",
      " [5.6 3.0 4.1 1.3 'Iris-versicolor']\n",
      " [5.5 2.5 4.0 1.3 'Iris-versicolor']\n",
      " [5.5 2.6 4.4 1.2 'Iris-versicolor']\n",
      " [6.1 3.0 4.6 1.4 'Iris-versicolor']\n",
      " [5.8 2.6 4.0 1.2 'Iris-versicolor']\n",
      " [5.0 2.3 3.3 1.0 'Iris-versicolor']\n",
      " [5.6 2.7 4.2 1.3 'Iris-versicolor']\n",
      " [5.7 3.0 4.2 1.2 'Iris-versicolor']\n",
      " [5.7 2.9 4.2 1.3 'Iris-versicolor']\n",
      " [6.2 2.9 4.3 1.3 'Iris-versicolor']\n",
      " [5.1 2.5 3.0 1.1 'Iris-versicolor']\n",
      " [5.7 2.8 4.1 1.3 'Iris-versicolor']\n",
      " [6.3 3.3 6.0 2.5 'Iris-virginica']\n",
      " [5.8 2.7 5.1 1.9 'Iris-virginica']\n",
      " [7.1 3.0 5.9 2.1 'Iris-virginica']\n",
      " [6.3 2.9 5.6 1.8 'Iris-virginica']\n",
      " [6.5 3.0 5.8 2.2 'Iris-virginica']\n",
      " [7.6 3.0 6.6 2.1 'Iris-virginica']\n",
      " [4.9 2.5 4.5 1.7 'Iris-virginica']\n",
      " [7.3 2.9 6.3 1.8 'Iris-virginica']\n",
      " [6.7 2.5 5.8 1.8 'Iris-virginica']\n",
      " [7.2 3.6 6.1 2.5 'Iris-virginica']\n",
      " [6.5 3.2 5.1 2.0 'Iris-virginica']\n",
      " [6.4 2.7 5.3 1.9 'Iris-virginica']\n",
      " [6.8 3.0 5.5 2.1 'Iris-virginica']\n",
      " [5.7 2.5 5.0 2.0 'Iris-virginica']\n",
      " [5.8 2.8 5.1 2.4 'Iris-virginica']\n",
      " [6.4 3.2 5.3 2.3 'Iris-virginica']\n",
      " [6.5 3.0 5.5 1.8 'Iris-virginica']\n",
      " [7.7 3.8 6.7 2.2 'Iris-virginica']\n",
      " [7.7 2.6 6.9 2.3 'Iris-virginica']\n",
      " [6.0 2.2 5.0 1.5 'Iris-virginica']\n",
      " [6.9 3.2 5.7 2.3 'Iris-virginica']\n",
      " [5.6 2.8 4.9 2.0 'Iris-virginica']\n",
      " [7.7 2.8 6.7 2.0 'Iris-virginica']\n",
      " [6.3 2.7 4.9 1.8 'Iris-virginica']\n",
      " [6.7 3.3 5.7 2.1 'Iris-virginica']\n",
      " [7.2 3.2 6.0 1.8 'Iris-virginica']\n",
      " [6.2 2.8 4.8 1.8 'Iris-virginica']\n",
      " [6.1 3.0 4.9 1.8 'Iris-virginica']\n",
      " [6.4 2.8 5.6 2.1 'Iris-virginica']\n",
      " [7.2 3.0 5.8 1.6 'Iris-virginica']\n",
      " [7.4 2.8 6.1 1.9 'Iris-virginica']\n",
      " [7.9 3.8 6.4 2.0 'Iris-virginica']\n",
      " [6.4 2.8 5.6 2.2 'Iris-virginica']\n",
      " [6.3 2.8 5.1 1.5 'Iris-virginica']\n",
      " [6.1 2.6 5.6 1.4 'Iris-virginica']\n",
      " [7.7 3.0 6.1 2.3 'Iris-virginica']\n",
      " [6.3 3.4 5.6 2.4 'Iris-virginica']\n",
      " [6.4 3.1 5.5 1.8 'Iris-virginica']\n",
      " [6.0 3.0 4.8 1.8 'Iris-virginica']\n",
      " [6.9 3.1 5.4 2.1 'Iris-virginica']\n",
      " [6.7 3.1 5.6 2.4 'Iris-virginica']\n",
      " [6.9 3.1 5.1 2.3 'Iris-virginica']\n",
      " [5.8 2.7 5.1 1.9 'Iris-virginica']\n",
      " [6.8 3.2 5.9 2.3 'Iris-virginica']\n",
      " [6.7 3.3 5.7 2.5 'Iris-virginica']\n",
      " [6.7 3.0 5.2 2.3 'Iris-virginica']\n",
      " [6.3 2.5 5.0 1.9 'Iris-virginica']\n",
      " [6.5 3.0 5.2 2.0 'Iris-virginica']\n",
      " [6.2 3.4 5.4 2.3 'Iris-virginica']\n",
      " [5.9 3.0 5.1 1.8 'Iris-virginica']]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# โหลดข้อมูลดอกไม้\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\sirintip\\\\Downloads\\\\IRIS.csv\")\n",
    "\n",
    "# แปลงข้อมูลประเภท DataFrame ให้เป็นข้อมูลประเภท numpy.ndarray เฉพาะคอลัมน์ sepal_length และ sepal_width\n",
    "data = data[[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species\"]].to_numpy()\n",
    "\n",
    "# แสดงข้อมูลประเภท numpy.ndarray\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ค่า λ ที่ดีที่สุดคือ: 0.1\n",
      "ความแม่นยำของโมเดลที่ดีที่สุดคือ: 0.16666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sirintip\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\sirintip\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\sirintip\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3588 (\\N{THAI CHARACTER KHO KHWAI}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\sirintip\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3656 (\\N{THAI CHARACTER MAI EK}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\sirintip\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3634 (\\N{THAI CHARACTER SARA AA}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# โหลดข้อมูลดอกไม้\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\sirintip\\\\Downloads\\\\IRIS.csv\")\n",
    "\n",
    "# ตรวจสอบว่าคอลัมน์ Species มีอยู่แล้วหรือไม่\n",
    "if \"Species\" not in data.columns:\n",
    "    data[\"Species\"] = data.pop(\"petal_length\")\n",
    "\n",
    "# แปลงเป้าหมายให้เป็นประเภทไบนารี\n",
    "le = LabelEncoder()\n",
    "data[\"Species\"] = le.fit_transform(data[\"Species\"])\n",
    "\n",
    "# แยกข้อมูลออกเป็นข้อมูลการฝึกและข้อมูลทดสอบ\n",
    "X_train, X_test, y_train, y_true = train_test_split(\n",
    "    data[[\"sepal_length\", \"sepal_width\"]].to_numpy(),\n",
    "    data[\"Species\"].to_numpy(),\n",
    "    test_size=0.4,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# กำหนดค่า λ ที่แตกต่างกัน\n",
    "lambdas = [0.1, 1, 10, 100, 1000]\n",
    "\n",
    "# ฝึกอบรมโมเดล\n",
    "models = []\n",
    "for lambda_ in lambdas:\n",
    "    model = LogisticRegression(penalty=\"l2\", C=1 / lambda_)\n",
    "    model.fit(X_train, y_train)\n",
    "    models.append(model)\n",
    "\n",
    "# ประเมินโมเดล\n",
    "accuracies = []\n",
    "for model in models:\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracies.append(accuracy_score(y_true, y_pred))\n",
    "\n",
    "# เลือกโมเดลที่ดีที่สุด\n",
    "best_model = models[np.argmax(accuracies)]\n",
    "\n",
    "# พิมพ์ข้อมูลสรุปของโมเดล\n",
    "print(\"ค่า λ ที่ดีที่สุดคือ:\", lambdas[np.argmax(accuracies)])\n",
    "print(\"ความแม่นยำของโมเดลที่ดีที่สุดคือ:\", accuracies[np.argmax(accuracies)])\n",
    "\n",
    "# บันทึกค่าความแม่นยำของโมเดลในแต่ละค่า λ ลงในไฟล์\n",
    "with open(\"results.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"λ\", \"accuracy\"])\n",
    "    for lambda_, accuracy in zip(lambdas, accuracies):\n",
    "        writer.writerow([lambda_, accuracy])\n",
    "\n",
    "# พิมพ์กราฟ\n",
    "plt.plot(lambdas, accuracies)\n",
    "plt.xlabel(\"ค่า λ\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.Test to see the effect of sampling proportion in Task 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1 Accuracy: 0.1\n",
      "Learning rate: 1 Accuracy: 0.1\n",
      "Learning rate: 1 Accuracy: 0.1\n",
      "Learning rate: 1 Accuracy: 0.1\n",
      "Learning rate: 1 Error: 0.9\n",
      "Learning rate: 1 Error: 0.9\n",
      "Learning rate: 1 Error: 0.9\n",
      "Learning rate: 1 Error: 0.9\n",
      "Learning rate: 1 Processing time: 0.015525341033935547\n",
      "Learning rate: 1 Processing time: 0.016999483108520508\n",
      "Learning rate: 1 Processing time: 0.018004655838012695\n",
      "Learning rate: 1 Processing time: 0.02099299430847168\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "# Determine different learning rates\n",
    "learning_rates = [0.001, 0.01, 0.1, 1]\n",
    "\n",
    "# Train models with different learning rates\n",
    "models = []\n",
    "for learning_rate in learning_rates:\n",
    "    model = LogisticRegression(penalty=\"l2\", C=1 / 10)\n",
    "    model.fit(X_train, y_train)\n",
    "    models.append(model)\n",
    "\n",
    "# Calculate model accuracy values\n",
    "for model in models:\n",
    "    accuracy = model.score(X_test, y_true)\n",
    "    print(\"Learning rate:\", learning_rate, \"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate model error values\n",
    "for model in models:\n",
    "    error_rate = 1 - model.score(X_test, y_true)\n",
    "    print(\"Learning rate:\", learning_rate, \"Error:\", error_rate)\n",
    "\n",
    "# Calculate the processing time of the model\n",
    "for model in models:\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    training_time = end - start\n",
    "    print(\"Learning rate:\", learning_rate, \"Processing time:\", training_time)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
